---
title: "Ne_sQTLv2 Notebook"
output: html_notebook
---
`#r format(Sys.time(), '%d %B, %Y')`

### 01/15/2020

Our present goal, as of the aforementioned date, is to find both splicing transcripts that are far more abundantly expressed in individuals who are at least heterozygous for the Neandertal allele where they have some (modest) transcript counts for the modern human allele (quantitative), and splicing transcripts that are altogether novel and uniquely expressed in individuals that are at least heterozygous for the Neandertal allele where they have 0 (zero) counts for the modern human allele. The findings from these two analyses will be presented in two different sections in the manuscript. 

What we have so far is a merged table with columns representing `variant_id`, `transcript_id`, genotype (so `0`, `1`, or `2` for homozygous modern human, heterozygous, and homozygous archaic, respectively), and `counts`. We pursued an avenue where we consolidated rows with identical `variant_id`/`transcript_id` pairs, filtered the tables, then made them horizontal, then we went back and did the same thing without filtering, found the difference in average counts per sample between the `HH` and `NN` transcripts, and looked for transcripts where there are fewer than 100 counts for the homozygous modern human genotype and greater than 250 counts for the homozygous archaic genotype.

Now, we are retooling our analysis. We're reconstructing our `merge_table` table to also include `sample_id`, so that we can filter instances where a homozygous modern human genotype results in the expression of 0 counts of a transcript **except** in one or two individuals who might have been misannotated, although, I'm not quite sure why we're doing that because we are going to discard those `variant_id`/`transcript_id` pairs anyway. But what we ultimately want is to

  1. Isolate instances where there is a novel NL transcript resulting from an sQTL (so no human expression but even some heterozygous NL expression)
  2. Isolate instances where there is a marked increase in gene expression in the NL genotypes

### 01/21/2020

From Steph:
```
I’m still trying to think of the best way to filter for interesting things in this data. I guess in the ideal situation we’d have:
- All HH samples are 0
- If a few HH samples are nonzero, their expression is very low
- All heterozygous human samples have nonzero expression
- All NLNL samples also have nonzero expression
but probably it’ll be easiest to just filter by the first criteria to begin with
```
I was able to regenerate the tables to include the individual ID (e.g. `GTEX-XXXX`) for each tissue. 

### 01/29/2020

I've spent hours agonizing over how to get the GTEX ID for each row to show up in `merge_tables.R` before realizing that it's not important right now. I'm going back to the horizontalized tables (pre-difference, I hope any of this makes sense when you read it) and I'm filtering for cases where all HH = 0 for any particular `variant_id` and `transcript_id` pair. Man I'm hungry right now.

Here's the path for the directory that is important and I should focus on: `/work-zfs/rmccoy22/aseyedi2/tempData/merge/old_attempt/final_iso/horizontal`

### 01/30/2020

Finally found those doggone Neandertal-specific isoforms. They're in `NoHHIsoforms.txt`. The code is in `test.R`. The names are bad, I know, but I just had to put down something because I have to go to the lab. We found over 16k sQTLs with 0 human transcripts and more than 0 Neandertal transcripts. This should be interesting. 

### 02/04/2020

Rajiv has asked me to use a software tool developed by Dr. Joshua Akey that does not require an outgroup population, unlike SPrime, as far as I can tell. In order to prep the GTEx VCF, I need to remove indels and multiallelic SNPs, which I will do using `vcftools`:

```
vcftools 
  --vcf {input} \
  --max-alleles 2 \
  --remove-indels \
  --recode \
  --out {output}
```

This is for the admixed MH VCF. I need to find the archaic VCF, but in order to do that, I have to find out more about it as an input into IBDMix.

In order to download the Neandertal VCF, I have to do the following:
```
ml vcftools 

for i in {1..22}; do
  wget "http://cdna.eva.mpg.de/neandertal/altai/AltaiNeandertal/VCF/AltaiNea.hg19_1000g.${i}.mod.vcf.gz"
done

for i in {1..22}; do
  gunzip AltaiNea.hg19_1000g.${i}.mod.vcf.gz
done

vcf-concat -f concatList.txt > AltaiNea.hg19_100g.concat.vcf

# now liftover the vcf
wget http://hgdownload.cse.ucsc.edu/goldenpath/hg19/liftOver/hg19ToHg38.over.chain.gz

vcftools --vcf AltaiNea.hg19_1000g.concat.vcf --remove-filtered "LowQual" --out AltaiNea.hg19_1000g.concat.LowQualRemoved

java -jar ~/work/progs/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar LiftoverVcf -I AltaiNea.hg19_1000g.concat.vcf -O AltaiNea.hg38_1000g.concat.vcf -C hg19ToHg38.over.chain.gz -R hg38.fa.gz --REJECT rejectedRecords.txt

```

Steph and I are also meeting with Rajiv later to discuss the results of the NL-specific sQTLs that we found. Apparently the results do not open an interesting line of inquiry according to Steph. We will see.

### 02/06/2020
Okay, I am working on the Josh Akey IBDMix pipeline, but now I have to liftover the archaic genomes which I now have over to `hg38`, and I'm realizing that in our pipeline we have an over_chain rule which downloads hg38 -> hg19 chain file:

```
rule over_chain:
    output:
        "metadata/hg38ToHg19.over.chain"
    params:
        url="https://hgdownload.soe.ucsc.edu/goldenPath/hg38/liftOver/hg38ToHg19.over.chain.gz"
    shell:
        "wget {params.url} metadata/;"
        "gunzip {output}.gz"
```

Not sure why that is and I can't remember why I included this rule. That's a bit of a bad sign. EDIT: okay now I am realizing that the reason I converted hg38 to hg19 is because I was converting the GTEx VCF to an earlier version to compare the SPrime results to hg19. I am pretty sure.

But now, as you can see in the previous entry (`02/04/2020`), I am working on a way to prepare the archaic genome for use with GTEx. This way makes the most sense. I'm going to use `gatk` again to produce a new hg38 Neandertal VCF.

### 02/07/2020

Okay I made some progress but I've hit another snag:

```
java -jar ~/work/progs/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar LiftoverVcf -I AltaiNea.hg19_1000g.concat.vcf -O AltaiNea.hg38_1000g.concat.vcf -C hg19ToHg38.over.chain -R hg38.fa --REJECT rejectedRecords.txt

Exception in thread "main" java.lang.IllegalArgumentException: Output format type is not set, or could not be inferred from the output path. If a path was used, does it have a valid VCF extension (.vcf, .vcf.gz, .bcf)?
        at htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder.build(VariantContextWriterBuilder.java:462)
        at htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder.build(VariantContextWriterBuilder.java:415)
        at picard.vcf.LiftoverVcf.doWork(LiftoverVcf.java:355)
        at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305)
        at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:103)
        at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:113)
```

I submitted a post on the GATK website after trying this with both GATK and Picard so we will see what they say. No idea what this error means.

#### update
Okay, I just found out it is because the `--REJECT` flag needs to be in `vcf` format

But now I'm getting a different problem:

```
java.lang.IllegalStateException: Key LowQual found in VariantContext field FILTER at 1:10014 but this key isn't defined in the VCFHeader.  We require all VCFs to have complete VCF headers by default.
```

The variant is messed up. I'm going to have to do something about it. Not sure what. I have a suspicion this has something to do with the fact that I concatenated the Neandertal VCFs. I'm going to try to run this on one chromosome individually.

```
java -jar ~/work/progs/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar LiftoverVcf -I vcfChrom/AltaiNea.hg19_1000g.17.mod.vcf -O AltaiNea.hg38_1000g.Chrom1.vcf -C hg19ToHg38.over.chain -R hg38.fa --REJECT rejectedRecords.vcf
```

Okay, nothing I do works, I have no idea why. I have to recreate the VCF. I'll talk to Rajiv about this next Monday.

### 02/10/2020

It's amazing what some time off can do. So the `FILTER` column have either `LowQual` values or `.` values. I want to remove all rows that have `LowQual` values.

I can use the following unix command: `sed -i '/LowQual/d' ./infile` but I'm going to try to use VCFtools, which is not working for some reason:

```
[aseyedi2@jhu.edu@rmccoy22-dev archaicGenomes]$ vcftools --vcf AltaiNea.hg19_1000g.concat.vcf --removed-filtered-all --out AltaiNea.hg19_1000g.concat.LowQualRemoved


Error: Unknown option: --removed-filtered-all0
```

lol I just realized as I pasted this that I spelled it `removed` and not `remove`:

`vcftools --vcf AltaiNea.hg19_1000g.concat.vcf --remove-filtered "LowQual" --out AltaiNea.hg19_1000g.concat.LowQualRemoved`

This is where it should be by now:

```
ml vcftools 

for i in {1..22}; do
  wget "http://cdna.eva.mpg.de/neandertal/altai/AltaiNeandertal/VCF/AltaiNea.hg19_1000g.${i}.mod.vcf.gz"
done

for i in {1..22}; do
  gunzip AltaiNea.hg19_1000g.${i}.mod.vcf.gz
done

vcf-concat -f concatList.txt > AltaiNea.hg19_100g.concat.vcf

# now liftover the vcf
wget http://hgdownload.cse.ucsc.edu/goldenpath/hg19/liftOver/hg19ToHg38.over.chain.gz

#vcftools --vcf AltaiNea.hg19_1000g.concat.vcf --remove-filtered "LowQual" --out AltaiNea.hg19_1000g.concat.LowQualRemoved

sed '/LowQual/d' ./AltaiNea.hg38_1000g.concat.vcf > AltaiNea.hg19_1000g.concat.LowQualRemoved.vcf

java -jar ~/work/progs/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar LiftoverVcf -I AltaiNea.hg19_1000g.concat.LowQualRemoved.vcf -O AltaiNea.hg38_1000g.concat.LowQualRemoved.vcf -C hg19ToHg38.over.chain -R hg38.fa --REJECT rejectedRecords.vcf
```

So I'm using `vcftools` to filter out non-PASS values now.

Okay, that didn't work for no good reason. The error message says:
```
After filtering, kept 1 out of 1 Individuals
After filtering, kept -1612752079 out of a possible -1611057532 Sites
File does not contain any sites
Run Time = 5418.00 seconds
```
This makes no sense and is insane to me. There are two types of values in the filter, one is `LowQual` but the other is `.`, which I understand to be "unknown." I think my problem has something to do with this. I'm going to try to fix it but I might just use Unix if I can't do it with VCFtools just to save time.

Here is the Unix command I would use: `sed '/LowQual/d' ./AltaiNea.hg38_1000g.concat.vcf > AltaiNea.hg19_1000g.concat.LowQualRemoved.vcf`

Okay. I'm still getting trouble. This time it's this:
```
[Mon Feb 10 14:36:03 EST 2020] Executing as aseyedi2@jhu.edu@rmccoy22-dev on Linux 3.10.0-862.14.4.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_181-b13; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.0.12.0
INFO    2020-02-10 14:36:03     LiftoverVcf     Loading up the target reference genome.
INFO    2020-02-10 14:36:16     LiftoverVcf     Lifting variants over and sorting (not yet writing the output file.)

[Mon Feb 10 14:59:31 EST 2020] picard.vcf.LiftoverVcf done. Elapsed time: 23.50 minutes.
Runtime.totalMemory()=7414480896
To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp
java.lang.IllegalStateException: Key . found in VariantContext field INFO at 1:121387974 but this key isn't defined in the VCFHeader.  We require all VCFs to have complete VCF headers by default.
        at htsjdk.variant.vcf.VCFEncoder.fieldIsMissingFromHeaderError(VCFEncoder.java:202)
        at htsjdk.variant.vcf.VCFEncoder.write(VCFEncoder.java:141)
        at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:248)
        at picard.vcf.LiftoverVcf.rejectVariant(LiftoverVcf.java:456)
        at picard.vcf.LiftoverVcf.doWork(LiftoverVcf.java:357)
        at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295)
        at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25)
        at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160)
        at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203)
        at org.broadinstitute.hellbender.Main.main(Main.java:289)
```
So now it's saying in the "INFO" field there is a dot but the dot isn't defined and that's a problem. I don't agree. I don't think it's necessarily a problem.

### 02/11/2020

Just figured out w Rajiv that we need to use the tag `--ALLOW_MISSING_FIELDS_IN_HEADER`. It was in the documentation. I should have looked at the documentation before posting about it, but oh well.
```
java -jar ~/work/progs/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar LiftoverVcf -I AltaiNea.hg19_1000g.concat.LowQualRemoved.vcf -O AltaiNea.hg38_1000g.concat.LowQualRemoved.vcf -C hg19ToHg38.over.chain -R hg38.fa --REJECT rejectedRecords.vcf --ALLOW_MISSING_FIELDS_IN_HEADER
```

Also, for finding the NL-specific isoforms with Steph, we are going back to square 1 again. This time, our goal is going to be to sum all of the samples where, for each transcript/variant pair's genotype, count = 0, and all of the samples where count is > 0. If this doesn't make sense, touch base again with Steph. It's something like that. But we're basically trying to find instances where +HH = 1 or 2 and -HH = 500 (so the number of individuals who have this transcript/variant pair and have the homozygous modern human genotype is less than 5 and the number of individuals who have this transcript/variant pair and something else is a bunch). I'm just going to work on this tomorrow when I'm a bit more fresh.

Okay, so I think I to alter the "merge_tables.R" file, that's how I am going to get what I want. Actually I already have something like that, but I'm not sure how I got it. I do not remember. It was only a couple of weeks ago. Alright. I think I will figure it outwhen I can. So the files I need to mess with are in `/scratch/groups/rmccoy22/aseyedi2/NL_sQTL_iso/merge/tables_with_sample_id`

btw `test.R` takes in a bunch of different files into one data table.

### 02/12/2020

GATK LiftoverVcf worked but also didn't work. Here is a sample of the output:

```
INFO    2020-02-11 11:50:04     LiftoverVcf     Loading up the target reference genome.
INFO    2020-02-11 11:50:19     LiftoverVcf     Lifting variants over and sorting (not yet writing the output file.)
INFO    2020-02-11 20:19:55     LiftoverVcf     Processed 2682215217 variants.
INFO    2020-02-11 20:19:55     LiftoverVcf     2682215217 variants failed to liftover.
INFO    2020-02-11 20:19:55     LiftoverVcf     0 variants lifted over but had mismatching reference alleles after lift over.
INFO    2020-02-11 20:19:55     LiftoverVcf     100.0000% of variants were not successfully lifted over and written to the output.
INFO    2020-02-11 20:19:55     LiftoverVcf     liftover success by source contig:
```

Rajiv is saying I should try using GRCh37 over chain instead. The URL is below:

`https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/gnomAD/b37ToHg38.over.chain`

So we are trying it again, but this time we are using the command:
```
java -jar ~/work/progs/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar LiftoverVcf -I AltaiNea.hg19_1000g.concat.LowQualRemoved.vcf -O AltaiNea.hg38_1000g.concat.LowQualRemoved.vcf -C b37ToHg38.over.chain -R hg38.fa --REJECT rejectedRecords.vcf --ALLOW_MISSING_FIELDS_IN_HEADER
```

